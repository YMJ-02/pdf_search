#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
memory_optimized_fix.py - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” ë° ëˆ„ìˆ˜ ìˆ˜ì •
"""

import numpy as np
from sentence_transformers import SentenceTransformer
import torch
import gc
import threading
import weakref
from typing import List, Union, Optional, Dict, Any
from dataclasses import dataclass

class UltraLightEmbedder:
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ê·¹ë„ë¡œ ìµœì í™”í•œ ì„ë² ë”© ìƒì„±ê¸°"""
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        """
        ì´ˆê²½ëŸ‰ ì„ë² ë”© ìƒì„±ê¸° ì´ˆê¸°í™”
        - all-MiniLM-L6-v2: ê°€ì¥ ê°€ë²¼ìš´ ëª¨ë¸ (23MB, 384ì°¨ì›)
        """
        if hasattr(self, '_initialized') and self._initialized:
            return
            
        print(f"ğŸ¤– ë¡œë”© ì¤‘: {model_name} (ê²½ëŸ‰ ëª¨ë¸)")
        
        # PyTorch ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
        torch.set_num_threads(1)  # ìŠ¤ë ˆë“œ 1ê°œë¡œ ì œí•œ
        torch.set_num_interop_threads(1)
        
        # ëª¨ë¸ ë¡œë“œ ì‹œ ë©”ëª¨ë¦¬ ìµœì í™” ì˜µì…˜
        self.model = SentenceTransformer(
            model_name,
            device='cpu',
            trust_remote_code=False
        )
        
        # ëª¨ë¸ì„ evaluation ëª¨ë“œë¡œ ì„¤ì •í•˜ê³  gradient ë¹„í™œì„±í™”
        self.model.eval()
        for param in self.model.parameters():
            param.requires_grad = False
        
        # ì„ë² ë”© ì°¨ì› í™•ì¸
        test_embedding = self._encode_single("test")
        self.embedding_dim = test_embedding.shape[0]
        
        print(f"âœ… ê²½ëŸ‰ ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ì°¨ì›: {self.embedding_dim}")
        
        self._initialized = True
        self._force_cleanup()
    
    def _encode_single(self, text: str) -> np.ndarray:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ì¸ì½”ë”© (ë©”ëª¨ë¦¬ ìµœì í™”)"""
        with torch.no_grad():
            embedding = self.model.encode(
                text,
                convert_to_numpy=True,
                normalize_embeddings=True,
                show_progress_bar=False,
                batch_size=1  # ë°°ì¹˜ í¬ê¸° 1ë¡œ ì œí•œ
            )
        
        # ì¦‰ì‹œ ë©”ëª¨ë¦¬ ì •ë¦¬
        self._force_cleanup()
        
        return embedding.astype(np.float32)  # ë©”ëª¨ë¦¬ ì ˆì•½
    
    def encode(self, texts: Union[str, List[str]], batch_size: int = 8) -> np.ndarray:
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°°ì¹˜ ì¸ì½”ë”©"""
        if isinstance(texts, str):
            return self._encode_single(texts)
        
        # ì‘ì€ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            
            with torch.no_grad():
                batch_embeddings = self.model.encode(
                    batch_texts,
                    convert_to_numpy=True,
                    normalize_embeddings=True,
                    show_progress_bar=False,
                    batch_size=len(batch_texts)
                )
            
            embeddings.append(batch_embeddings.astype(np.float32))
            
            # ë°°ì¹˜ë§ˆë‹¤ ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬
            self._force_cleanup()
        
        if embeddings:
            result = np.vstack(embeddings)
            # ìµœì¢… ì •ë¦¬
            self._force_cleanup()
            return result
        
        return np.array([])
    
    def _force_cleanup(self):
        """ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()  # ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
    
    def __del__(self):
        """ì†Œë©¸ìì—ì„œ ê°•ì œ ë©”ëª¨ë¦¬ í•´ì œ"""
        if hasattr(self, 'model'):
            del self.model
        self._force_cleanup()

class MemoryEfficientDocumentIndex:
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì— íŠ¹í™”ëœ ë¬¸ì„œ ì¸ë±ìŠ¤"""
    
    def __init__(self, max_documents: int = 1000):
        self.max_documents = max_documents
        self.documents: List[Dict[str, Any]] = []  # ë©”íƒ€ë°ì´í„°ë§Œ ì €ì¥
        self.embeddings: Optional[np.ndarray] = None
        
        # ì‹±ê¸€í†¤ ì„ë² ë” ì‚¬ìš©
        self.embedder = UltraLightEmbedder()
        
        print(f"ğŸ“š ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì¸ë±ìŠ¤ ìƒì„± (ìµœëŒ€ {max_documents:,}ê°œ)")
    
    def add_document(self, doc_id: str, content: str, metadata: Optional[Dict] = None) -> bool:
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë¬¸ì„œ ì¶”ê°€"""
        if len(self.documents) >= self.max_documents:
            return False
        
        # ì¤‘ë³µ ID ì²´í¬
        if any(doc['id'] == doc_id for doc in self.documents):
            return False
        
        # ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì €ì¥ (ë‚´ìš©ì€ ì„ë² ë”©ìœ¼ë¡œë§Œ ì €ì¥)
        doc_info = {
            'id': doc_id,
            'content': content,  # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í•„ìš”í•˜ë¯€ë¡œ ì €ì¥
            'metadata': metadata or {}
        }
        self.documents.append(doc_info)
        
        # ì„ë² ë”© ìƒì„± ë° ì¶”ê°€
        new_embedding = self.embedder.encode(content)
        
        if self.embeddings is None:
            self.embeddings = new_embedding.reshape(1, -1)
        else:
            self.embeddings = np.vstack([self.embeddings, new_embedding.reshape(1, -1)])
        
        # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬
        if len(self.documents) % 50 == 0:  # ë” ìì£¼ ì •ë¦¬
            gc.collect()
        
        return True
    
    def search(self, query: str, top_k: int = 5, min_similarity: float = 0.3) -> List[Dict]:
        """ê³ ì„±ëŠ¥ ë²¡í„° ê²€ìƒ‰"""
        if not self.documents or self.embeddings is None:
            return []
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.embedder.encode(query)
        
        # ë²¡í„°í™”ëœ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = np.dot(self.embeddings, query_embedding)
        
        # ì„ê³„ê°’ í•„í„°ë§
        valid_indices = np.where(similarities >= min_similarity)[0]
        
        if len(valid_indices) == 0:
            return []
        
        # ìƒìœ„ kê°œ ì„ íƒ
        valid_similarities = similarities[valid_indices]
        top_indices = valid_indices[np.argsort(valid_similarities)[::-1][:top_k]]
        
        # ê²°ê³¼ êµ¬ì„±
        results = []
        for idx in top_indices:
            doc = self.documents[idx]
            results.append({
                'id': doc['id'],
                'content': doc['content'],
                'similarity_score': float(similarities[idx]),
                'metadata': doc['metadata']
            })
        
        return results
    
    def get_memory_usage(self) -> Dict[str, float]:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰"""
        import psutil
        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024
        
        embedding_size_mb = 0
        if self.embeddings is not None:
            embedding_size_mb = self.embeddings.nbytes / 1024 / 1024
        
        return {
            'total_memory_mb': memory_mb,
            'embedding_cache_mb': embedding_size_mb,
            'document_count': len(self.documents)
        }
    
    def clear_all(self):
        """ëª¨ë“  ë°ì´í„° ì •ë¦¬"""
        self.documents.clear()
        self.embeddings = None
        gc.collect()

def test_memory_optimized_system():
    """ë©”ëª¨ë¦¬ ìµœì í™”ëœ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    import psutil
    import time
    
    print("ğŸ§ª ë©”ëª¨ë¦¬ ìµœì í™” ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 40)
    
    process = psutil.Process()
    initial_memory = process.memory_info().rss / 1024 / 1024
    print(f"ì´ˆê¸° ë©”ëª¨ë¦¬: {initial_memory:.1f} MB")
    
    # ì¸ë±ìŠ¤ ìƒì„±
    index = MemoryEfficientDocumentIndex(max_documents=100)
    after_init = process.memory_info().rss / 1024 / 1024
    print(f"ì¸ë±ìŠ¤ ìƒì„± í›„: {after_init:.1f} MB (+{after_init - initial_memory:.1f} MB)")
    
    # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì¶”ê°€
    test_docs = [
        ("doc1", "ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ìˆ  ê°œë°œ"),
        ("doc2", "ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ í”„ë¡ íŠ¸ì—”ë“œ êµ¬ì¶•"),
        ("doc3", "ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ ë° ìµœì í™”"),
        ("doc4", "í´ë¼ìš°ë“œ ì¸í”„ë¼ êµ¬ì¶• ê°€ì´ë“œ"),
        ("doc5", "ëª¨ë°”ì¼ ì•± ê°œë°œ í”„ë ˆì„ì›Œí¬"),
    ]
    
    print(f"\nğŸ“š {len(test_docs)}ê°œ ë¬¸ì„œ ì¶”ê°€...")
    
    for doc_id, content in test_docs:
        success = index.add_document(doc_id, content)
        if success:
            current_memory = process.memory_info().rss / 1024 / 1024
            print(f"  {doc_id}: {current_memory:.1f} MB")
    
    final_memory = process.memory_info().rss / 1024 / 1024
    print(f"\në¬¸ì„œ ì¶”ê°€ ì™„ë£Œ: {final_memory:.1f} MB (+{final_memory - initial_memory:.1f} MB)")
    
    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    queries = ["ì¸ê³µì§€ëŠ¥", "ì›¹ê°œë°œ", "ë°ì´í„°ë² ì´ìŠ¤"]
    
    print(f"\nğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:")
    for query in queries:
        start_time = time.time()
        results = index.search(query, top_k=3)
        search_time = time.time() - start_time
        
        print(f"  '{query}': {len(results)}ê°œ ê²°ê³¼, {search_time:.4f}ì´ˆ")
        for result in results:
            print(f"    - {result['id']}: {result['similarity_score']:.3f}")
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´
    memory_info = index.get_memory_usage()
    print(f"\nğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
    for key, value in memory_info.items():
        if 'mb' in key:
            print(f"  {key}: {value:.2f} MB")
        else:
            print(f"  {key}: {value}")
    
    return final_memory - initial_memory

if __name__ == "__main__":
    memory_increase = test_memory_optimized_system()
    
    print(f"\nğŸ¯ ìµœì¢… ê²°ê³¼:")
    if memory_increase < 100:  # 100MB ì´í•˜
        print(f"âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” ì„±ê³µ: +{memory_increase:.1f} MB")
    else:
        print(f"âš ï¸  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°œì„  í•„ìš”: +{memory_increase:.1f} MB")